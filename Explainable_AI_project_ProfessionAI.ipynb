{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flaaa31/Explainable-AI-project/blob/main/Explainable_AI_project_ProfessionAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretazione di una rete neurale per compliance normativa in ambito bancario\n",
        "Banca Virtuosa, istituto di riferimento nel settore finanziario, ha identificato l'esigenza di migliorare la trasparenza e la comprensibilità dei modelli di intelligenza artificiale utilizzati nei propri sistemi. Per raggiungere questo obiettivo, Banca Virtuosa ha lanciato un progetto mirato all'implementazione di tecniche di Explainable AI (XAI), in conformità con la normativa vigente sulla trasparenza bancaria.\n",
        "\n",
        "Attualmente, Banca Virtuosa utilizza modelli di classificazione pre-addestrati per analizzare e classificare dati finanziari critici. Tuttavia, la mancanza di trasparenza nelle decisioni di questi modelli può compromettere la fiducia dei clienti e limitare la capacità della banca di migliorare i propri sistemi in modo mirato. Identificare e correggere gli errori di classificazione è cruciale per garantire accuratezza e affidabilità nei servizi offerti.\n",
        "\n",
        "**Benefici della Soluzione**\n",
        "\n",
        "1. **Trasparenza nelle Decisioni del Modello**: Implementando tecniche di XAI come Grad-CAM, LIME, SHAP, Integrated Gradients e Occlusion Maps, Banca Virtuosa sarà in grado di generare mappe di salienza che mostrano visivamente quali elementi influenzano le decisioni del modello. Questo incremento di trasparenza migliorerà la fiducia dei clienti e degli stakeholder, dimostrando l'affidabilità e la spiegabilità delle operazioni del sistema di classificazione.\n",
        "2. **Miglioramento Continuo delle Performance**: Analizzando le mappe di salienza, Banca Virtuosa potrà identificare con precisione le aree in cui il modello commette errori, sia nelle classificazioni corrette che in quelle errate. Questa analisi dettagliata permetterà di apportare miglioramenti mirati al modello, ottimizzando le sue performance e riducendo il rischio di interpretazioni errate dei dati.\n",
        "3. **Conformità Normativa**: Il progetto garantirà che le decisioni dei modelli di intelligenza artificiale siano spiegabili, in linea con i requisiti normativi vigenti. La trasparenza delle decisioni AI è essenziale per la conformità normativa e la governance aziendale, particolarmente in settori regolamentati come quello finanziario.\n",
        "4. **Promozione dell'Innovazione**: L'utilizzo di tecniche avanzate di XAI all'interno di Banca Virtuosa promuoverà l'innovazione nel campo dell'intelligenza artificiale. Questo rafforzerà la posizione della banca come pioniere nell'adozione di tecnologie avanzate, consentendo di offrire ai clienti soluzioni sempre più sofisticate e affidabili.\n",
        "**Dettagli del Progetto**\n",
        "\n",
        "Fase 1: Utilizzo di un Modello di Classificazione Pre-Addestrato\n",
        "* Modello: Utilizzare un modello pre-addestrato, come DenseNet, dalla libreria torchvision.\n",
        "* Dataset: Applicare il modello a un dataset di immagini, ad esempio MNIST, per esplorare le sue decisioni di classificazione.\n",
        "\n",
        "Fase 2: Generazione di Mappe di Salienza\n",
        "* Tecniche di XAI: Implementare tecniche come Grad-CAM, LIME, SHAP, Integrated Gradients e Occlusion Maps per generare mappe di salienza del modello.\n",
        "\n",
        "Fase 3: Report Finale\n",
        "* Descrizione del Dataset: Dettagliare l'origine, la struttura e le caratteristiche del dataset utilizzato.\n",
        "* Analisi delle Mappe di Salienza: Confrontare le mappe di salienza per classi corrette ed errate per identificare e comprendere gli errori del modello.\n",
        "* Sistema Spiegabile (Opzionale): Descrivere un sistema completamente spiegabile che potrebbe eseguire la stessa classificazione, offrendo ulteriori insights sulle decisioni del modello.\n",
        "\n",
        "**Obiettivi del Progetto**\n",
        "\n",
        "1. Comprensione del Modello: Utilizzare tecniche di XAI per ottenere una comprensione approfondita del funzionamento interno del modello pre-addestrato.\n",
        "2. Visualizzazione delle Decisioni: Visualizzare in modo chiaro e interpretabile quali elementi influenzano le decisioni del modello attraverso le mappe di salienza.\n",
        "3. Identificazione degli Errori: Analizzare le mappe di salienza per identificare e comprendere gli errori del modello, distinguendo tra classificazioni corrette ed errate.\n",
        "4. Creazione di Sistemi Spiegabili: Se possibile, sviluppare o descrivere un sistema completamente spiegabile che possa effettuare la stessa classificazione, fornendo ulteriori insights sulle decisioni del modello.\n",
        "\n",
        "**Motivazione del Progetto**  \n",
        "Le tecniche di Explainable AI sono essenziali per Banca Virtuosa per migliorare la trasparenza, ottimizzare le performance dei modelli e garantire la conformità normativa. Con questo progetto, la banca mira a rafforzare la fiducia dei clienti, migliorare l'efficienza operativa e promuovere l'innovazione nel campo dell'intelligenza artificiale."
      ],
      "metadata": {
        "id": "cRRpyWTI1jyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1: Training a deep learning model for a classification task (in our case, handwritten digits classification)"
      ],
      "metadata": {
        "id": "bE_1tL__FFbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and install modules"
      ],
      "metadata": {
        "id": "sVGu3Q_snq1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install captum lime"
      ],
      "metadata": {
        "id": "PNC3FD6tzBZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from captum.attr import Saliency, Occlusion, GradientShap, IntegratedGradients, LayerGradCam\n",
        "from lime import lime_image\n",
        "from lime.wrappers.scikit_image import SegmentationAlgorithm\n",
        "from skimage.color import gray2rgb, label2rgb\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shap\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree, _tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "bmD8f9abnqMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device setting\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "id": "TGx8VwfV7oy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Download (MNIST)"
      ],
      "metadata": {
        "id": "O-x_WEkMi12Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# standard images transformation (normalization and tensor conversion)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ],
      "metadata": {
        "id": "JfROB87VP69u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WX2TU_381jDt"
      },
      "outputs": [],
      "source": [
        "# Train and test dataset download (MNIST)\n",
        "train_dataset = datasets.MNIST(root='./data', train=True,\n",
        "                              download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False,\n",
        "                             download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# images shape\n",
        "print(f\"Train dataset images shape: {train_dataset[0][0].shape}\")\n",
        "print(f\"Test dataset images shape: {test_dataset[0][0].shape}\")"
      ],
      "metadata": {
        "id": "0O1_HeRy7P1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "3uPDkGGVunB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset dimensions\n",
        "print(f\"Train dataset shape: {len(train_dataset)} images\")\n",
        "print(f\"Test dataset shape: {len(test_dataset)} images\")"
      ],
      "metadata": {
        "id": "L1A4h-BrlTYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# labels\n",
        "classes = train_dataset.classes\n",
        "classes"
      ],
      "metadata": {
        "id": "q2Pq_ci5q2C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting an image from tensor to numpy format for viewing purposes\n",
        "def back_to_image(img):\n",
        "    img = img / 2 + 0.5\n",
        "    npimg = img.numpy()\n",
        "    return np.transpose(npimg, (1, 2, 0))"
      ],
      "metadata": {
        "id": "tsj9cP1-qbh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show random images belonging to dataset\n",
        "\n",
        "def print_random_images(dataset, num_rows, num_cols, class_names):\n",
        "    num_images = num_rows * num_cols\n",
        "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(num_cols * 2, num_rows * 2))\n",
        "\n",
        "    for i in range(num_rows):\n",
        "        for j in range(num_cols):\n",
        "            index = random.randint(0, len(dataset) - 1)  # Random image selected\n",
        "            image = back_to_image(dataset[index][0])  # Random image converted\n",
        "            label = dataset[index][1]  # class\n",
        "            axs[i, j].imshow(image, cmap = \"gray\") # black and white images\n",
        "            axs[i, j].set_title(class_names[label])  # show class name\n",
        "            axs[i, j].axis('off')  # hide axis\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "print_random_images(train_dataset, num_rows=2, num_cols=5, class_names=classes)  # Print 10 trainset random images, 2 lines and 5 columns"
      ],
      "metadata": {
        "id": "hbQJhWd-pFy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if classes are balanced\n",
        "\n",
        "train_labels = [label for _, label in train_dataset]\n",
        "test_labels = [label for _, label in test_dataset]\n",
        "\n",
        "train_label_counts = {label: train_labels.count(label) for label in range(10)}\n",
        "test_label_counts = {label: test_labels.count(label) for label in range(10)}\n",
        "\n",
        "print(\"Train Label Counts:\", train_label_counts)\n",
        "print(\"Test Label Counts:\", test_label_counts)"
      ],
      "metadata": {
        "id": "f4wH2fcDqiY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot of classes distribution\n",
        "\n",
        "# adding dataset names, for viewing purposes\n",
        "train_dataset.name = \"Train\"\n",
        "test_dataset.name = \"Test\"\n",
        "\n",
        "def plot_label_counts(dataset):\n",
        "\n",
        "  labels = [label for _, label in dataset]\n",
        "  label_counts = {label: labels.count(label) for label in range(10)}\n",
        "\n",
        "  # label counts plot\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  bars = plt.bar(label_counts.keys(), label_counts.values(),\n",
        "                color='steelblue', alpha=0.7, edgecolor='black', linewidth=0.5)\n",
        "\n",
        "  # Adding values up\n",
        "  for bar in bars:\n",
        "      height = bar.get_height()\n",
        "      plt.text(bar.get_x() + bar.get_width()/2., height + 10,\n",
        "              f'{int(height)}', ha='center', va='bottom')\n",
        "\n",
        "  plt.xlabel('Label')\n",
        "  plt.ylabel('Number of samples')\n",
        "  plt.title(f\"Class Distribution in '{dataset.name}' Dataset\")\n",
        "  plt.xticks(range(10))  # Show all ticks in X-axis\n",
        "  plt.grid(True, alpha=0.3)\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "8udkdLkirvA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_label_counts(train_dataset)"
      ],
      "metadata": {
        "id": "3QFwAeSNsYcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_label_counts(test_dataset)"
      ],
      "metadata": {
        "id": "MvAuem_staur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model selection and performances evaluation using a neural network\n",
        "architecture taken from https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/\n",
        "\n",
        "(It's not the best model you could possibly use, but it has some decent performance and it's fast to train, the purpose of this project is to focus on XAI techniques, so the model used is almost irrelevant, you can change it with something else, if you want)"
      ],
      "metadata": {
        "id": "nSCKg7g3uwhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTBaseline(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTBaseline, self).__init__()\n",
        "        # Conv2D(32, (3,3)) -> Conv2d(1, 32, kernel_size=3)\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=0)\n",
        "        # MaxPooling2D((2,2)) -> MaxPool2d(2)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        # Dense(100) -> Linear(13*13*32, 100)\n",
        "        self.fc1 = nn.Linear(13*13*32, 100)  # 26x26 -> 13x13 after pooling\n",
        "        # Dense(10) -> Linear(100, 10)\n",
        "        self.fc2 = nn.Linear(100, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Input x shape: [N, 1, 28, 28]\n",
        "      # N = Batch size (number of images in the batch)\n",
        "      # 1 = Color channels (since the images are grayscale)\n",
        "      # 28, 28 = Height and width of the image in pixels\n",
        "\n",
        "      # First convolutional layer + ReLU activation function\n",
        "      x = F.relu(self.conv1(x)) # [N, 32, 26, 26]\n",
        "\n",
        "      # First Max Pooling layer\n",
        "      x = self.pool1(x) # [N, 32, 13, 13]\n",
        "\n",
        "      # Flattening the tensor to prepare it for the fully connected (dense) layers\n",
        "      x = x.view(-1, 32 * 13 * 13) #[N, 5408]\n",
        "\n",
        "      # First fully connected (dense) layer + ReLU activation function\n",
        "      x = F.relu(self.fc1(x)) # [N, 128]\n",
        "\n",
        "      # The softmax function to get the final probabilities will be applied within the loss function\n",
        "      x = self.fc2(x) # [N, 10]\n",
        "\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "mfp3jaI5fUeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPARING EVERYTHING WE NEED FOR TRAINING\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# model, loss, optimizer\n",
        "model = MNISTBaseline().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n"
      ],
      "metadata": {
        "id": "-C2FLvlBfYPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 1. training and testing functions\n",
        "def train_epoch(model, device, train_loader, optimizer, criterion, epoch):\n",
        "    \"\"\"\n",
        "    Training function for one epoch.\n",
        "\n",
        "    Args:\n",
        "    - model: PyTorch model to train.\n",
        "    - device: PyTorch device (e.g., 'cuda' or 'cpu').\n",
        "    - train_loader: DataLoader for training data.\n",
        "    - optimizer: PyTorch optimizer.\n",
        "    - criterion: PyTorch loss function.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = output.max(1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    avg_loss = train_loss / len(train_loader)\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def test(model, device, test_loader, criterion):\n",
        "\n",
        "    \"\"\"\n",
        "    Test function.\n",
        "\n",
        "    Args:\n",
        "    - model: PyTorch model to test.\n",
        "    - device: PyTorch device (e.g., 'cuda' or 'cpu').\n",
        "    - test_loader: DataLoader for test data.\n",
        "    - criterion: PyTorch loss function.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    print(f\"\\nTest set loss: {test_loss:.4f}\")\n",
        "\n",
        "    return test_loss, accuracy\n"
      ],
      "metadata": {
        "id": "Wte86xMVhQZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# option 2. loading a pre-trained model from google drive (if you already have one)\n",
        "def load_mnist_model(path, model_name='mnist_model.pth'):\n",
        "    \"\"\"\n",
        "    Automatically loads a pre-trained model from Google Drive.\n",
        "\n",
        "    Args:\n",
        "    - path: Path to the model file saved on Google Drive.\n",
        "    - model_name: Name of the model file to load.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Mounting drive if not already mounted\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    # model path\n",
        "    model_path = f'{path}/{model_name}' # remember to change model_name and path with yours\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Model not found in: {model_path}\")\n",
        "\n",
        "    # Model loading\n",
        "    model = MNISTBaseline()  # Class definition\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Model '{model_name}' loaded!\")\n",
        "    print(f\"Test set Accuracy: {checkpoint['test_accuracy']:.2f}%\")\n",
        "\n",
        "    return model, checkpoint"
      ],
      "metadata": {
        "id": "3fJTI7cjVVCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHOOSING IF TRAINING A MODEL OR LOADING A PRE-TRAINED ONE\n",
        "\n",
        "do_train = False # I already trained a model, if not, set this to True, it only takes a few minutes with colab free GPU\n",
        "\n",
        "if do_train:\n",
        "\n",
        "  # Training for 10 epochs\n",
        "  num_epochs = 10\n",
        "  train_losses, train_accuracies = [], []\n",
        "  test_losses, test_accuracies = [], []\n",
        "\n",
        "  print(\"Training started...\")\n",
        "  print(\"=\" * 50)\n",
        "\n",
        "  for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "      # Train\n",
        "      train_loss, train_acc = train_epoch(model, device, train_loader, optimizer, criterion, epoch)\n",
        "      train_losses.append(train_loss)\n",
        "      train_accuracies.append(train_acc)\n",
        "\n",
        "      # Test\n",
        "      test_loss, test_acc = test(model, device, test_loader, criterion)\n",
        "      test_losses.append(test_loss)\n",
        "      test_accuracies.append(test_acc)\n",
        "\n",
        "      # Update learning rate\n",
        "      scheduler.step()\n",
        "\n",
        "      print(f\"Epoch {epoch}: Train Accuracy: {train_acc:.2f}%, Test Accuracy: {test_acc:.2f}%\")\n",
        "      print(\"-\" * 50)\n",
        "\n",
        "  print(\"Training Completed!\")\n",
        "\n",
        "  # Saving model\n",
        "  torch.save({\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'test_accuracy': test_accuracies[-1],\n",
        "      'train_accuracy': train_accuracies[-1],\n",
        "      'epoch': num_epochs\n",
        "  }, 'mnist_model.pth')\n",
        "\n",
        "  print(f\"\\nModel saved! \\nAccuracy: {test_accuracies[-1]:.2f}%\")\n",
        "\n",
        "  # Training plot\n",
        "  plt.figure(figsize=(12, 4))\n",
        "\n",
        "  # Loss plot\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(range(1, num_epochs + 1), train_losses, 'b-', label='Train Loss')\n",
        "  plt.plot(range(1, num_epochs + 1), test_losses, 'r-', label='Test Loss')\n",
        "  plt.title('Training loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "  # Accuracy plot\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(range(1, num_epochs + 1), train_accuracies, 'b-', label='Train Accuracy')\n",
        "  plt.plot(range(1, num_epochs + 1), test_accuracies, 'r-', label='Test Accuracy')\n",
        "  plt.title('Training Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy (%)')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "else:\n",
        "  # loading model\n",
        "  model, checkpoint = load_mnist_model(path=\"/content/drive/MyDrive/ProfessionAI/Master AI Engineer/11. XAI/progetto\") # change with your path"
      ],
      "metadata": {
        "id": "i1A9DkYRhTb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFUSION MATRIX\n",
        "model.eval()\n",
        "\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Get the model's outputs (logits)\n",
        "        output = model(data)\n",
        "\n",
        "        # Get the predicted class (the one with the highest score)\n",
        "        pred = output.argmax(dim=1, keepdim=True).cpu().numpy()\n",
        "        y_pred.extend(pred.flatten()) # Adding predictions\n",
        "\n",
        "        # Adding true labels\n",
        "        y_true.extend(target.cpu().numpy().flatten())\n",
        "\n",
        "\n",
        "# 1. Classification Report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_true, y_pred, target_names=classes))\n",
        "\n",
        "# some separation\n",
        "print(\"-----------------------------------------------------------\")\n",
        "\n",
        "# 2. Confusion Matrix\n",
        "print(\"\\n\\nConfusion Matrix:\\n\")\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel('Predicted Label', fontweight='bold')\n",
        "plt.ylabel('True Label', fontweight='bold')\n",
        "plt.title('Confusion Matrix', fontsize = 13, fontweight=\"bold\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VznEbcvzSeyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check model performances on random examples\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # test batch\n",
        "    data_iter = iter(test_loader)\n",
        "    images, labels = next(data_iter)\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Predictions of first 10 images\n",
        "    outputs = model(images[:10])\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    # Showing results\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
        "    for i in range(10):\n",
        "        ax = axes[i//5, i%5]\n",
        "        # Denormalization for viewing purposes: from [-1,1] to [0,1]\n",
        "        img = images[i].cpu().squeeze() * 0.5 + 0.5\n",
        "        ax.imshow(img, cmap='gray')\n",
        "        color = \"green\" if predicted[i].item() == labels[i].item() else \"red\"\n",
        "        ax.set_title(f'True: {labels[i].item()}\\nPred: {predicted[i].item()}', color = color)\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "sicei4Zhhw-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2: XAI analysis\n",
        "In this phase, we will examine XAI techniques, progressing from the simplest to the most sophisticated.\n",
        "For every technique, we will discuss about pros and cons, especially in our use case."
      ],
      "metadata": {
        "id": "gCQ_6KLTqGoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "first, let's define some utility functions that we'll call during most of XAI techniques analysis"
      ],
      "metadata": {
        "id": "OSC2SugkFfbt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea6778cd"
      },
      "source": [
        "# 1. Function for image preprocessing\n",
        "def preprocess_image_for_xai(image_tensor):\n",
        "    \"\"\"\n",
        "    Preprocesses a single image tensor for XAI techniques and visualization.\n",
        "\n",
        "    Args:\n",
        "        image_tensor (torch.Tensor): The input image tensor (shape [1, 28, 28] or [28, 28]).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - img_tensor_unsqueezed (torch.Tensor): Image tensor unsqueezed for model input ([1, 1, 28, 28]).\n",
        "            - orig_img_np (np.ndarray): Original image as a numpy array for display (denormalized, [28, 28]).\n",
        "            - image_rgb_uint8 (np.ndarray): Original image as a uint8 RGB numpy array ([28, 28, 3]).\n",
        "    \"\"\"\n",
        "    # Ensure tensor has a batch dimension\n",
        "    if image_tensor.ndim == 3:\n",
        "        img_tensor_unsqueezed = image_tensor.unsqueeze(0)\n",
        "    elif image_tensor.ndim == 2:\n",
        "        img_tensor_unsqueezed = image_tensor.unsqueeze(0).unsqueeze(0)\n",
        "    else:\n",
        "        img_tensor_unsqueezed = image_tensor # correct shape [1, 1, 28, 28]\n",
        "\n",
        "    # Denormalize for visualization: from [-1,1] to [0,1]\n",
        "    orig_img_np = img_tensor_unsqueezed.squeeze().cpu().detach().numpy() * 0.5 + 0.5\n",
        "\n",
        "    # Convert to RGB uint8\n",
        "    image_rgb = gray2rgb(orig_img_np)\n",
        "    image_rgb_uint8 = (image_rgb * 255).astype(np.uint8)\n",
        "\n",
        "    return img_tensor_unsqueezed, orig_img_np, image_rgb_uint8\n",
        "\n",
        "\n",
        "# 2. Function for getting model predictions\n",
        "def get_model_prediction(model, image_tensor):\n",
        "    \"\"\"\n",
        "    Gets the model prediction for a single image tensor.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The PyTorch model.\n",
        "        image_tensor (torch.Tensor): The input image tensor with batch dimension ([1, 1, 28, 28]).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - predicted_class (int): The predicted class index.\n",
        "            - confidence (float): The confidence score for the predicted class.\n",
        "            - output (torch.Tensor): The raw model output (logits).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    image_tensor = image_tensor.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)\n",
        "        predicted_class = output.argmax(dim=1).item()\n",
        "        confidence = F.softmax(output, dim=1)[0, predicted_class].item()\n",
        "\n",
        "    return predicted_class, confidence, output\n",
        "\n",
        "\n",
        "# 3. Visualization function\n",
        "def display_xai_explanation(original_image_np, attribution_map, true_label, predicted_class, technique_title,\n",
        "                             cmap='hot', overlay_alpha=0.5, show_colorbar=True, filter_percentile=None,\n",
        "                             use_divergent=False):\n",
        "    \"\"\"\n",
        "    Displays the original image, the attribution map, and their overlay.\n",
        "\n",
        "    Args:\n",
        "        original_image_np (np.ndarray): The original image numpy array (denormalized, [28, 28]).\n",
        "        attribution_map (np.ndarray): The saliency/attribution map ([28, 28]).\n",
        "        true_label (int): The true class label.\n",
        "        predicted_class (int): The predicted class label.\n",
        "        technique_title (str): Name of the XAI technique.\n",
        "        cmap (str, optional): Colormap for the attribution map. Defaults to 'hot'.\n",
        "        overlay_alpha (float, optional): Transparency for the overlay. Defaults to 0.5.\n",
        "        show_colorbar (bool, optional): Whether to display a colorbar. Defaults to True.\n",
        "        filter_percentile (float, optional): If provided, filters the attribution map\n",
        "                                             to show only values above this percentile (0-100), for cleaner visualizations.\n",
        "        use_divergent (bool, optional): If True, preserves positive and negative values for divergent colormaps.\n",
        "                                       If False, uses absolute values (default behavior). Defaults to False.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Handle divergent vs non-divergent visualization\n",
        "    if use_divergent:\n",
        "        # For divergent colormaps (like RdBu_r), preserve positive and negative values\n",
        "        if filter_percentile is not None:\n",
        "            # For filtering with divergent maps, use absolute value for thresholding but preserve signs\n",
        "            abs_map = np.abs(attribution_map)\n",
        "            threshold = np.percentile(abs_map, filter_percentile)\n",
        "            important_mask = abs_map > threshold\n",
        "            display_map = np.zeros_like(attribution_map)\n",
        "            if important_mask.any():\n",
        "                # Keep original signs but normalize the important values\n",
        "                important_abs_values = abs_map[important_mask]\n",
        "                scale_factor = (important_abs_values - threshold) / (important_abs_values.max() - threshold + 1e-8)\n",
        "                display_map[important_mask] = np.sign(attribution_map[important_mask]) * scale_factor\n",
        "        else:\n",
        "            # Use original values, ensuring symmetric range around zero\n",
        "            display_map = attribution_map.copy()\n",
        "\n",
        "        # Set symmetric color limits for divergent colormap\n",
        "        abs_max = max(abs(display_map.min()), abs(display_map.max()))\n",
        "        vmin, vmax = -abs_max, abs_max\n",
        "\n",
        "    else:\n",
        "        # Original behavior for non-divergent techniques (Grad-CAM, etc.)\n",
        "        if filter_percentile is not None:\n",
        "            abs_map = np.abs(attribution_map)\n",
        "            threshold = np.percentile(abs_map, filter_percentile)\n",
        "            important_mask = abs_map > threshold\n",
        "            cleaned_map = np.zeros_like(abs_map)\n",
        "            if important_mask.any():\n",
        "                # Normalize the important values between 0 and 1 for better visualization\n",
        "                important_values = abs_map[important_mask]\n",
        "                cleaned_map[important_mask] = (important_values - threshold) / (important_values.max() - threshold + 1e-8)\n",
        "            display_map = cleaned_map\n",
        "        else:\n",
        "            # Normalize the raw map for consistent color scaling\n",
        "            abs_map = np.abs(attribution_map)\n",
        "            if abs_map.max() > abs_map.min():\n",
        "                display_map = (abs_map - abs_map.min()) / (abs_map.max() - abs_map.min())\n",
        "            else:\n",
        "                display_map = abs_map\n",
        "\n",
        "        # Use natural min/max for non-divergent colormaps\n",
        "        vmin, vmax = display_map.min(), display_map.max()\n",
        "\n",
        "    # 1. Original Image\n",
        "    color = \"green\" if predicted_class == true_label else \"red\"\n",
        "    axes[0].imshow(original_image_np, cmap='gray')\n",
        "    axes[0].set_title(f'Original Image\\nTrue: {true_label}, Pred: {predicted_class}', color=color)\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # 2. Saliency/Attribution Map\n",
        "    im = axes[1].imshow(display_map, cmap=cmap, vmin=vmin, vmax=vmax, interpolation='nearest')\n",
        "    axes[1].set_title(f'{technique_title} Map')\n",
        "    axes[1].axis('off')\n",
        "    if show_colorbar:\n",
        "        fig.colorbar(im, ax=axes[1])\n",
        "\n",
        "    # 3. Overlay\n",
        "    axes[2].imshow(original_image_np, cmap='gray')\n",
        "    im_overlay = axes[2].imshow(display_map, cmap=cmap, vmin=vmin, vmax=vmax, alpha=overlay_alpha, interpolation='nearest')\n",
        "    axes[2].set_title(f'{technique_title} Overlay')\n",
        "    axes[2].axis('off')\n",
        "    if show_colorbar:\n",
        "        fig.colorbar(im_overlay, ax=axes[2])\n",
        "\n",
        "    plt.suptitle(f'Explanation with {technique_title}', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 4. Function to pick random images from a dataloader\n",
        "def pick_random_images_from_loader(dataloader, num_images):\n",
        "    \"\"\"\n",
        "    Picks random images and labels from a DataLoader.\n",
        "\n",
        "    Args:\n",
        "        dataloader (torch.utils.data.DataLoader): The DataLoader.\n",
        "        num_images (int): Number of images to pick.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - images (torch.Tensor): Stacked image tensors.\n",
        "            - labels (torch.Tensor): Stacked label tensors.\n",
        "    \"\"\"\n",
        "    all_imgs, all_labs = [], []\n",
        "    for images, labels in dataloader:\n",
        "        all_imgs.append(images)\n",
        "        all_labs.append(labels)\n",
        "        if len(all_imgs) * dataloader.batch_size >= num_images:\n",
        "            break\n",
        "    all_imgs = torch.cat(all_imgs, dim=0)\n",
        "    all_labs = torch.cat(all_labs, dim=0)\n",
        "    indices = random.sample(range(len(all_imgs)), k=min(num_images, len(all_imgs)))\n",
        "    return all_imgs[indices], all_labs[indices]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - LIME (Local Interpretable Model-agnostic Explanations)\n",
        "\n",
        "Answer to the question:\n",
        "- Which parts of this specific image were the most important for the model's prediction?\n",
        "\n",
        "How does it work?\n",
        "- It works by creating an \"explainer\", a simpler, interpretable model that approximates the complex model's behavior for a single prediction.\n",
        "\n",
        "How it Works: The Core Idea\n",
        "\n",
        "1. Segment the Image: First, it breaks the image into smaller, understandable pieces called \"superpixels.\"\n",
        "\n",
        "2. Create Perturbations: It then generates thousands of new, slightly different images by randomly hiding some of these superpixels.\n",
        "\n",
        "3. Learn from Predictions: For each new image, it gets the original model's prediction. By observing how the prediction changes when certain superpixels are hidden, LIME learns which ones are most influential.\n",
        "\n",
        "4. Finally, it trains a simple, interpretable model (like a linear regression) on these results. The weights of this simple model tell us which superpixels from the original image had the most positive (or negative) impact on the final prediction.\n",
        "\n",
        "Pros:\n",
        "- Model-Agnostic: It can explain any image classification model, regardless of its architecture. You don't need to know how the \"black box\" works.\n",
        "\n",
        "- Human-Interpretable: The output is a visual heatmap, making it easy to see what the model \"looked at.\"\n",
        "\n",
        "\n",
        "Cons:\n",
        "- Local Fidelity Only: The explanation is only valid for that one specific image. It doesn't explain the model's overall behavior.\n",
        "\n",
        "- Explanation Instability: The result can vary depending on the segmentation algorithm used and the random nature of the perturbations.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The quality of a LIME explanation heavily depends on how the image is broken down into superpixels. A good segmentation helps LIME ask more meaningful questions. We will explore three common methods:\n",
        "\n",
        "- Quickshift: Tends to create segments that follow the natural contours and shapes in the image.\n",
        "\n",
        "- SLIC: Produces segments that are relatively uniform and compact, almost like a mosaic.\n",
        "\n",
        "- Grid: The simplest method. It just divides the image into a uniform grid, which is often surprisingly effective for centered objects like MNIST digits."
      ],
      "metadata": {
        "id": "RzZw3Rks8oWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_lime_helpers(model, segmentation_method='quickshift', grid_size=7):\n",
        "    \"\"\"\n",
        "    Creates helper functions needed for LIME analysis.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The PyTorch model.\n",
        "        segmentation_method (str, optional): Segmentation method for LIME. Defaults to 'quickshift'.\n",
        "        grid_size (int, optional): Grid size, only for 'grid' segmentation. Defaults to 7.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - explainer: LIME explainer object.\n",
        "            - classifier_fn: Wrapper for model predictions.\n",
        "            - segmenter: Segmentation algorithm.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    explainer = lime_image.LimeImageExplainer(verbose=False)\n",
        "\n",
        "    # Segmentation algorithm setup\n",
        "    if segmentation_method == 'slic':\n",
        "        segmenter = SegmentationAlgorithm('slic', n_segments=50, compactness=1, sigma=0.5)\n",
        "    elif segmentation_method == 'grid':\n",
        "        def grid_segmentation(image, grid_size=grid_size):\n",
        "            h, w = image.shape[:2]\n",
        "            segments = np.zeros((h, w), dtype=int)\n",
        "            segment_id = 0\n",
        "            step_h, step_w = h // grid_size, w // grid_size\n",
        "            for i in range(0, h, step_h):\n",
        "                for j in range(0, w, step_w):\n",
        "                    end_i, end_j = min(i + step_h, h), min(j + step_w, w)\n",
        "                    segments[i:end_i, j:end_j] = segment_id\n",
        "                    segment_id += 1\n",
        "            return segments\n",
        "        segmenter = grid_segmentation\n",
        "    else:  # 'quickshift' default\n",
        "        segmenter = SegmentationAlgorithm('quickshift', kernel_size=1, max_dist=50, ratio=0.1)\n",
        "\n",
        "    def classifier_fn(images_np):\n",
        "        \"\"\"\n",
        "        Wrapper for model predictions.\n",
        "\n",
        "        Args:\n",
        "            images_np (np.ndarray): Input images as a numpy array.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Model predictions.\n",
        "        \"\"\"\n",
        "        imgs_gray = images_np[..., 0].astype(np.float32) / 255.0\n",
        "        imgs_norm = (imgs_gray - 0.5) / 0.5\n",
        "        tensor = torch.from_numpy(imgs_norm).unsqueeze(1).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(tensor)\n",
        "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
        "        return probs\n",
        "\n",
        "    return explainer, classifier_fn, segmenter\n",
        "\n",
        "\n",
        "def get_lime_explanation(explainer, image_rgb_uint8, classifier_fn, segmenter,\n",
        "                        predicted_class, num_features=15, min_weight=0.01,\n",
        "                        num_samples=3000):\n",
        "    \"\"\"\n",
        "    Generates LIME explanation for a single image.\n",
        "\n",
        "    Args:\n",
        "        explainer: LIME explainer object\n",
        "        image_rgb_uint8: Image as a uint8 RGB numpy array\n",
        "        classifier_fn: Wrapper for model predictions\n",
        "        segmenter: Segmentation algorithm\n",
        "        predicted_class: Predicted class\n",
        "        num_features: Number of features for LIME\n",
        "        min_weight: Min weight for LIME\n",
        "        num_samples: Number of samples for LIME\n",
        "\n",
        "    Returns:\n",
        "        tuple: (explanation, temp, mask_pos)\n",
        "    \"\"\"\n",
        "\n",
        "    explanation = explainer.explain_instance(\n",
        "        image_rgb_uint8,\n",
        "        classifier_fn=classifier_fn,\n",
        "        top_labels=10,\n",
        "        hide_color=0,\n",
        "        num_samples=num_samples,\n",
        "        segmentation_fn=segmenter\n",
        "    )\n",
        "\n",
        "    temp, mask_pos = explanation.get_image_and_mask(\n",
        "        predicted_class,\n",
        "        positive_only=True,\n",
        "        num_features=num_features,\n",
        "        hide_rest=False,\n",
        "        min_weight=min_weight\n",
        "    )\n",
        "\n",
        "    return explanation, temp, mask_pos\n",
        "\n",
        "# visualization function personalized for LIME because it produces a different output (binary mask with segments)\n",
        "\n",
        "def visualize_lime_explanation(image_rgb_uint8, mask_pos, true_label,\n",
        "                              pred_class, segmentation_method):\n",
        "    \"\"\"\n",
        "    Show the original image, the mask, and the overlay.\n",
        "\n",
        "    Args:\n",
        "        image_rgb_uint8: Image as a uint8 RGB numpy array\n",
        "        mask_pos: Binary mask with segments\n",
        "        true_label: True label\n",
        "        pred_class: Predicted class\n",
        "        segmentation_method: Segmentation method (quickshift, slic or grid)\n",
        "\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "    # 1. Original image\n",
        "    color = 'green' if true_label == pred_class else 'red'\n",
        "    axes[0].imshow(image_rgb_uint8)\n",
        "    axes[0].set_title(f'Originale\\nTrue: {true_label}, Pred: {pred_class}', color=color)\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # 2. Positive zones\n",
        "    axes[1].imshow(label2rgb(mask_pos, bg_label=0, colors=['red']))\n",
        "    axes[1].set_title('Zone Positive')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # 3. Overlay\n",
        "    overlay = label2rgb(mask_pos, image_rgb_uint8, bg_label=0, alpha=0.4, colors=['red'])\n",
        "    axes[2].imshow(overlay)\n",
        "    axes[2].set_title('Overlay Zone Positive')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.suptitle(f'LIME with {segmentation_method.upper()} segmentation algorhitm', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def analyze_single_image_lime(model, img_tensor, true_label, explainer,\n",
        "                             classifier_fn, segmenter, segmentation_method,\n",
        "                             **lime_params):\n",
        "    \"\"\"\n",
        "    Analysis of a single image with LIME.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        img_tensor: Input image tensor\n",
        "        true_label: True label\n",
        "        explainer: LIME explainer object\n",
        "        classifier_fn: Wrapper for model predictions\n",
        "        segmenter: Segmentation algorithm\n",
        "        segmentation_method: Segmentation method\n",
        "        **lime_params: Additional parameters for LIME\n",
        "\n",
        "    Returns:\n",
        "        tuple: (explanation, mask)\n",
        "    \"\"\"\n",
        "    # Preprocessing image\n",
        "    img_tensor_unsqueezed, orig_img_np, image_rgb_uint8 = preprocess_image_for_xai(img_tensor)\n",
        "\n",
        "    # Model prediction\n",
        "    predicted_class, confidence, _ = get_model_prediction(model, img_tensor_unsqueezed)\n",
        "\n",
        "    # LIME explanation\n",
        "    explanation, temp, mask_pos = get_lime_explanation(\n",
        "        explainer, image_rgb_uint8, classifier_fn, segmenter,\n",
        "        predicted_class, **lime_params\n",
        "    )\n",
        "\n",
        "    # Visualization\n",
        "    visualize_lime_explanation(image_rgb_uint8, mask_pos, true_label,\n",
        "                              predicted_class, segmentation_method)\n",
        "\n",
        "    return explanation, mask_pos\n",
        "\n",
        "\n",
        "def analyze_images_with_lime(model, test_loader, num_images=3,\n",
        "                           segmentation_method='quickshift', grid_size=7,\n",
        "                           num_features=15, min_weight=0.01, num_samples=3000):\n",
        "    \"\"\"\n",
        "    Main function to analyze images with LIME.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        test_loader: DataLoader for test data\n",
        "        num_images: Number of images to analyze\n",
        "        segmentation_method: Segmentation method\n",
        "        grid_size: Grid size for 'grid' segmentation\n",
        "        num_features: Number of features for LIME\n",
        "        min_weight: Min weight for LIME\n",
        "        num_samples: Number of samples for LIME\n",
        "\n",
        "    Returns:\n",
        "        list: List of dictionaries containing explanations and masks.\n",
        "    \"\"\"\n",
        "    # Setup helper LIME\n",
        "    explainer, classifier_fn, segmenter = make_lime_helpers(\n",
        "        model, segmentation_method=segmentation_method, grid_size=grid_size\n",
        "    )\n",
        "\n",
        "    # LIME parameters\n",
        "    lime_params = {\n",
        "        'num_features': num_features,\n",
        "        'min_weight': min_weight,\n",
        "        'num_samples': num_samples\n",
        "    }\n",
        "\n",
        "    # Random images\n",
        "    imgs, labels = pick_random_images_from_loader(test_loader, num_images)\n",
        "\n",
        "    results = []\n",
        "    for i in range(len(imgs)):\n",
        "        img_tensor = imgs[i]\n",
        "        true_label = labels[i].item()\n",
        "\n",
        "        explanation, mask = analyze_single_image_lime(\n",
        "            model, img_tensor, true_label, explainer, classifier_fn,\n",
        "            segmenter, segmentation_method, **lime_params\n",
        "        )\n",
        "\n",
        "        results.append({\n",
        "            'explanation': explanation,\n",
        "            'mask': mask,\n",
        "            'true_label': true_label\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Let's try all the three segmentation algorhitms\n",
        "results_quickshift = analyze_images_with_lime(\n",
        "    model, test_loader, num_images=3, segmentation_method='quickshift'\n",
        ")\n",
        "\n",
        "results_slic = analyze_images_with_lime(\n",
        "    model, test_loader, num_images=3, segmentation_method='slic'\n",
        ")\n",
        "\n",
        "results_grid = analyze_images_with_lime(\n",
        "    model, test_loader, num_images=3, segmentation_method='grid',\n",
        "    grid_size=7, num_features=20\n",
        ")"
      ],
      "metadata": {
        "id": "04GVdidGKoUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - SHAP (SHapley Additive exPlanations)\n",
        "\n",
        "Answer to the question:\n",
        "- How much does each feature contribute to the difference between the model’s current prediction and its average prediction, according to cooperative game theory?\n",
        "\n",
        "The key idea is inspired by the Shapley values from game theory: each feature is treated as a “player” in a coalition, and SHAP assigns an importance value ensuring fairness and symmetry. The result is an additive decomposition, where the sum of the feature attributions exactly matches the difference between the model prediction and the baseline (usually, the mean prediction).\n",
        "\n",
        "Pros:\n",
        "- Consistent, theoretically grounded attributions (Shapley values).\n",
        "\n",
        "- Additive explanations: the sum of all feature effects explains the full prediction difference.\n",
        "\n",
        "- Works for both linear and complex models.\n",
        "\n",
        "Cons:\n",
        "- Classic SHAP (KernelSHAP) is slow on complex datasets or images, as it considers many possible feature combinations.\n",
        "\n",
        "- Standard implementations may not scale to large models or datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "exjUb3-J-R8M"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df555b0b"
      },
      "source": [
        "def shap_kernel_analysis(model, test_loader, n_samples=3, n_background=50, n_samples_shap=500, filter_percentile=None):\n",
        "    \"\"\"\n",
        "    Function that uses SHAP Kernel analysis to explain model predictions.\n",
        "\n",
        "    Args:\n",
        "    - model: PyTorch model to explain.\n",
        "    - test_loader: DataLoader for test data.\n",
        "    - n_samples: Number of samples to analyze.\n",
        "    - n_background: Number of background samples for SHAP.\n",
        "    - n_samples_shap: Number of samples for SHAP explainer.\n",
        "    - filter_percentile (float, optional): Percentile threshold for visualization filtering.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    # function to pick random images\n",
        "    all_images, all_labels = pick_random_images_from_loader(test_loader, n_samples + n_background)\n",
        "\n",
        "    # Separating test and background data\n",
        "    test_images = all_images[:n_samples]\n",
        "    test_labels = all_labels[:n_samples]\n",
        "    background_images = all_images[n_samples:n_samples + n_background]\n",
        "\n",
        "    # Preparing background data for SHAP (flattened numpy array)\n",
        "    background_data_flat = background_images.cpu().numpy().reshape(n_background, -1)\n",
        "\n",
        "    # Wrapper model for SHAP (expects flattened numpy input, returns probabilities)\n",
        "    def shap_predict_fn(x_flat):\n",
        "        \"\"\"\n",
        "        Predicts probabilities for input flattened numpy array.\n",
        "\n",
        "        Args:\n",
        "        - x_flat (np.ndarray): Flattened input data ([N, H*W]).\n",
        "\n",
        "        Returns:\n",
        "        - probabilities (np.ndarray): Predicted probabilities ([N, num_classes]).\n",
        "        \"\"\"\n",
        "        # Reshape flattened input back to image tensor format [N, 1, H, W]\n",
        "        x_tensor = torch.FloatTensor(x_flat).reshape(-1, 1, 28, 28).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(x_tensor)\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "        return probabilities.cpu().numpy()\n",
        "\n",
        "    # SHAP KernelExplainer\n",
        "    explainer = shap.KernelExplainer(shap_predict_fn,\n",
        "                                     background_data_flat,\n",
        "                                     link=\"identity\" # helps with stability\n",
        "                                     )\n",
        "\n",
        "    # Analyze each test image\n",
        "    for i in range(n_samples):\n",
        "        img_tensor = test_images[i]\n",
        "        true_label = test_labels[i].item()\n",
        "\n",
        "        # image preprocessing\n",
        "        img_tensor_unsqueezed, orig_img_np, _ = preprocess_image_for_xai(img_tensor)\n",
        "\n",
        "        # model prediction\n",
        "        predicted_class, confidence, _ = get_model_prediction(model, img_tensor_unsqueezed)\n",
        "\n",
        "        print(f\"Processing image {i+1}/{n_samples} - True: {true_label}, Predicted: {predicted_class}\")\n",
        "\n",
        "        # Calculate SHAP values for the flattened image\n",
        "        img_flat = img_tensor_unsqueezed.cpu().numpy().reshape(1, -1)\n",
        "\n",
        "        shap_values = explainer.shap_values(img_flat,\n",
        "                                            nsamples=n_samples_shap,\n",
        "                                             l1_reg=\"num_features(10)\" # regulatization improves stability\n",
        "                                            )\n",
        "\n",
        "        # Extract SHAP values for the predicted class and reshape back to 2D\n",
        "        if isinstance(shap_values, list):\n",
        "            # For multi-output models, shap_values is a list of arrays, one for each output\n",
        "            shap_2d = shap_values[predicted_class][0].reshape(28, 28)\n",
        "        else:\n",
        "            # For single-output models, shap_values is a single array\n",
        "            shap_2d = shap_values[0, :, predicted_class].reshape(28, 28)\n",
        "\n",
        "\n",
        "        # Visualizing results\n",
        "        # SHAP values can be positive or negative, used a diverging colormap like 'RdBu_r'\n",
        "        display_xai_explanation(\n",
        "            original_image_np=orig_img_np,\n",
        "            attribution_map=shap_2d,\n",
        "            true_label=true_label,\n",
        "            predicted_class=predicted_class,\n",
        "            technique_title='Kernel SHAP',\n",
        "            cmap='RdBu_r',\n",
        "            overlay_alpha=0.7,\n",
        "            show_colorbar=True,\n",
        "            filter_percentile=filter_percentile, # Pass filter setting\n",
        "            use_divergent=True # preserves positive and negative values for divergent colormaps\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "shap_results_kernel = shap_kernel_analysis(model, test_loader, n_samples=3, n_background=50, n_samples_shap=100)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - GradientSHAP\n",
        "GradientSHAP is a SHAP-inspired variant designed specifically for differentiable models like neural networks, it often gives more stable and faster results for image data.\n",
        "\n",
        "How does it work?\n",
        "GradientSHAP combines two main ideas:\n",
        "\n",
        "SmoothGrad: Adds noise to the input and averages predictions/gradients, yielding less noisy and more robust attributions.\n",
        "\n",
        "Integrated Gradients: Interpolates between a baseline (like a black or noisy image) and the actual image, computing gradients at steps along the path and summing them.\n",
        "\n",
        "Effectively, for each sample, GradientSHAP generates many “noisy” versions of the input and computes the average gradient-based attributions along a path from the baseline to the actual image. This produces attributions that are more reliable than raw gradients, reducing noise and instability.\n",
        "\n",
        "How is it different from classic SHAP?\n",
        "GradientSHAP leverages automatic differentiation in deep learning models; classic SHAP (like KernelSHAP) is model-agnostic and works by sampling over feature subsets.\n",
        "\n",
        "Performance: Much faster and more practical for neural networks, as it avoids the combinatorial explosion of subsets.\n",
        "\n",
        "Baseline Sampling: GradientSHAP depends on picking a baseline and sampling from noise, unlike classic SHAP, which considers all feature combinations.\n",
        "\n",
        "**Pros :)**\n",
        "- Fast and scalable for deep learning models.\n",
        "\n",
        "- More stable attributions than simple (vanilla) gradients.\n",
        "\n",
        "- Blends SHAP principles with Integrated Gradients and SmoothGrad.\n",
        "\n",
        "- Built directly into Captum, so it's easy to use with PyTorch workflows.\n",
        "\n",
        "**Cons :(**\n",
        "- The choice of baseline can affect interpretation.\n",
        "\n",
        "- Less purely “game theoretical” than KernelSHAP since it doesn't actually enumerate all feature subsets, but rather interpolates between baseline and input.\n",
        "\n",
        "- Requires differentiable models. (like NNs)\n",
        "\n",
        "- Noisy, as all the gradient methods"
      ],
      "metadata": {
        "id": "V1wDPnPHvFXJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49d6b652"
      },
      "source": [
        "def captum_gradient_shap_analysis(model, test_loader, n_samples=3, n_baselines_gs=10, n_samples_gs=50, filter_percentile=None):\n",
        "    \"\"\"\n",
        "    Uses GradientSHAP to explain model predictions.\n",
        "\n",
        "    Args:\n",
        "    - model: PyTorch model to explain.\n",
        "    - test_loader: DataLoader for test data.\n",
        "    - n_samples: Number of samples to analyze.\n",
        "    - n_baselines_gs: Number of baselines for GradientSHAP.\n",
        "    - n_samples_gs: Number of interpolation steps for GradientSHAP.\n",
        "    - filter_percentile (float, optional): Percentile threshold for visualization filtering.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    # pick random images\n",
        "    images, labels = pick_random_images_from_loader(test_loader, n_samples)\n",
        "\n",
        "    # Setup GradientShap\n",
        "    gradient_shap = GradientShap(model)\n",
        "\n",
        "    # Prepare baselines\n",
        "    baselines_gs = torch.randn_like(images[:n_baselines_gs]) * 0.1\n",
        "    baselines_gs = baselines_gs.to(device) # Move baselines to device\n",
        "\n",
        "\n",
        "    # Analyze each test image\n",
        "    for i in range(n_samples):\n",
        "        img_tensor = images[i]\n",
        "        true_label = labels[i].item()\n",
        "\n",
        "        # image preprocessing\n",
        "        img_tensor_unsqueezed, orig_img_np, _ = preprocess_image_for_xai(img_tensor)\n",
        "        img_tensor_unsqueezed = img_tensor_unsqueezed.to(device) # Move input tensor to the correct device\n",
        "\n",
        "\n",
        "        # model prediction\n",
        "        predicted_class, confidence, _ = get_model_prediction(model, img_tensor_unsqueezed)\n",
        "\n",
        "        # Calculate GradientShap attribution\n",
        "        attribution = gradient_shap.attribute(\n",
        "            img_tensor_unsqueezed,\n",
        "            baselines=baselines_gs[:min(n_baselines_gs, len(images))], # Use a subset of baselines if fewer images picked\n",
        "            target=predicted_class,\n",
        "            n_samples=n_samples_gs\n",
        "        )\n",
        "\n",
        "        # Visualize the results\n",
        "        # GradientSHAP attributions can be positive or negative, so we use RdBu_r again\n",
        "        display_xai_explanation(\n",
        "            original_image_np=orig_img_np, # Use the preprocessed numpy image\n",
        "            attribution_map=attribution.squeeze().cpu().detach().numpy(), # Use absolute values and ensure 2D numpy\n",
        "            true_label=true_label,\n",
        "            predicted_class=predicted_class,\n",
        "            technique_title='GradientSHAP',\n",
        "            cmap='RdBu_r',\n",
        "            overlay_alpha=0.5,\n",
        "            show_colorbar=True,\n",
        "            filter_percentile=filter_percentile, # Pass filter setting\n",
        "            use_divergent=True # preserves positive and negative values for divergent colormaps\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "captum_gradient_shap_analysis(model, test_loader, n_samples=3, filter_percentile=80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saliency maps\n",
        "**question to answer:** Which are the most important parts of an image for the inference?\n",
        "\n",
        "**Pros :)**\n",
        "- Computational Efficiency: Extremely fast to compute\n",
        "\n",
        "- Implementation Simplicity\n",
        "\n",
        "- Universal Applicability: Works with any neural network architecture without modification\n",
        "\n",
        "- Very Precise\n",
        "\n",
        "\n",
        "**Cons :(**\n",
        "\n",
        "- High Noise: Produces visually noisy maps due to irrelevant features passing through ReLU activations\n",
        "\n",
        "- Sensitivity to Perturbations: Small input changes can dramatically alter saliency maps, indicating poor robustness\n",
        "\n",
        "- Class Similarity Problem: Often produces similar-looking maps across different classes, especially on simple tasks\n",
        "\n",
        "- Limited Interpretability: Provides a narrow view of decision-making that may not reflect the model's actual reasoning process\n",
        "\n",
        "- Background Feature Interference: Highlights irrelevant background regions that don't contribute to the classification decision\n",
        "\n"
      ],
      "metadata": {
        "id": "MH-9MloIsedq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4044b417"
      },
      "source": [
        "def visualize_batch_saliency(model, test_loader, n_samples=4, filter_percentile=None):\n",
        "    \"\"\"\n",
        "    Visualize saliency maps.\n",
        "\n",
        "    Args:\n",
        "        model: Pytorch model\n",
        "        test_loader: DataLoader for test data\n",
        "        n_samples: Number of samples to visualize\n",
        "        filter_percentile (float, optional): Percentile threshold for visualization filtering.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Use reusable function to pick random images from test loader\n",
        "    images, labels = pick_random_images_from_loader(test_loader, n_samples)\n",
        "\n",
        "    # Setup Saliency\n",
        "    saliency = Saliency(model)\n",
        "\n",
        "    # Analyze each selected image\n",
        "    for i in range(min(n_samples, len(images))):\n",
        "        img_tensor = images[i]\n",
        "        true_label = labels[i].item() # Convert label tensor to int\n",
        "\n",
        "        # Image preprocessing\n",
        "        img_tensor_unsqueezed, orig_img_np, _ = preprocess_image_for_xai(img_tensor)\n",
        "        input_img = img_tensor_unsqueezed.requires_grad_(True).to(device) # Move input tensor to the correct device\n",
        "\n",
        "\n",
        "        # Model prediction\n",
        "        predicted_class, confidence, _ = get_model_prediction(model, input_img) # Use input_img with grad enabled\n",
        "\n",
        "\n",
        "        # Calculate Saliency attribution\n",
        "        saliency_attr = saliency.attribute(input_img, target=predicted_class)\n",
        "\n",
        "        # Visualize the results\n",
        "\n",
        "        display_xai_explanation(\n",
        "            original_image_np=orig_img_np, # Use the preprocessed numpy image\n",
        "            attribution_map=saliency_attr.abs().squeeze().cpu().detach().numpy(), # Use absolute values and ensure 2D numpy\n",
        "            true_label=true_label,\n",
        "            predicted_class=predicted_class,\n",
        "            technique_title='Saliency Map',\n",
        "            cmap='hot',\n",
        "            overlay_alpha=0.5,\n",
        "            show_colorbar=True,\n",
        "            filter_percentile=filter_percentile, # Pass filter setting\n",
        "            use_divergent=False # Saliency map is tipically positive\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "visualize_batch_saliency(model, test_loader, n_samples=4, filter_percentile=80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Integrated Gradients\n",
        "idea: overlap the gradients, retrieve saliency doing different gradients overlapping, summed they reduced the overall noise\n",
        "\n",
        "Pros:   \n",
        "- It works on any type of model\n",
        "\n",
        "Cons:\n",
        "- random background choice, different path with different backgrounds\n",
        "- not as much noisy as saliency map but still too noisy\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zzx9GyL85W_Y"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d44f5dff"
      },
      "source": [
        "def visualize_integrated_gradients_examples(model, test_loader, n_samples=4, n_steps=50, filter_percentile=None):\n",
        "    \"\"\"\n",
        "    Function that uses Integrated Gradients to explain model predictions.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model to explain.\n",
        "        test_loader: DataLoader for test data.\n",
        "        n_samples: Number of samples to analyze.\n",
        "        n_steps: Number of interpolation steps for Integrated Gradients.\n",
        "        filter_percentile (float, optional): Percentile threshold for visualization filtering.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # pick random images\n",
        "    images, labels = pick_random_images_from_loader(test_loader, n_samples)\n",
        "\n",
        "    # Integrated Gradients initialization\n",
        "    ig = IntegratedGradients(model)\n",
        "\n",
        "    # Analyze each selected image\n",
        "    for i in range(min(n_samples, len(images))):\n",
        "        img_tensor = images[i]\n",
        "        true_label = labels[i].item() # Convert label tensor to int\n",
        "\n",
        "        # image preprocessing\n",
        "        img_tensor_unsqueezed, orig_img_np, _ = preprocess_image_for_xai(img_tensor)\n",
        "        img_tensor_unsqueezed = img_tensor_unsqueezed.to(device) # Move input tensor to the correct device\n",
        "\n",
        "        # model prediction\n",
        "        predicted_class, confidence, _ = get_model_prediction(model, img_tensor_unsqueezed)\n",
        "\n",
        "        print(f\"Processing image {i+1}/{n_samples} - True: {true_label}, Predicted: {predicted_class}\")\n",
        "\n",
        "        # Calculate Integrated Gradients attribution\n",
        "        # Using a black image as baseline (-1.0 after normalization)\n",
        "        baseline = torch.zeros_like(img_tensor_unsqueezed).to(device) - 1.0\n",
        "        ig_attr = ig.attribute(\n",
        "            img_tensor_unsqueezed,\n",
        "            baselines=baseline,\n",
        "            target=predicted_class,\n",
        "            n_steps=n_steps\n",
        "        )\n",
        "\n",
        "        # visualize the results\n",
        "        display_xai_explanation(\n",
        "            original_image_np=orig_img_np, # Use the preprocessed numpy image\n",
        "            attribution_map=ig_attr.abs().squeeze().cpu().detach().numpy(), # Use absolute values and ensure 2D numpy\n",
        "            true_label=true_label,\n",
        "            predicted_class=predicted_class,\n",
        "            technique_title='Integrated Gradients',\n",
        "            cmap='hot', # if use_divergent = True, it's better to use \"RdBu_r\"\n",
        "            overlay_alpha=0.5,\n",
        "            show_colorbar=True,\n",
        "            filter_percentile=filter_percentile, # Pass filter setting\n",
        "            use_divergent=False # IG attributions can be positive or negative, but abs is often used for visualization, so we keep only positive (if changed to True, it's better to change also the cmap)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "visualize_integrated_gradients_examples(model, test_loader, n_samples=4, filter_percentile=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - Occlusion maps\n",
        "\n",
        "**question to answer:** How much \"damage\" to model performance I do if I cover this image specific region?\n",
        "\n",
        "\n",
        "**Pros :)**  \n",
        "\n",
        "- Model-Agnostic: Works with any model architecture\n",
        "\n",
        "- Robust Results: It makes model learning more robust (resolving possible dependencies from some pixels)-> good augmentation method\n",
        "\n",
        "- Clean Visualizations\n",
        "\n",
        "**Cons :(**\n",
        "\n",
        "- Slow: model called for every image part I'm considering\n"
      ],
      "metadata": {
        "id": "Iy69P5vT3FI6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e10e5bd8"
      },
      "source": [
        "def visualize_batch_occlusion(model, test_loader, n_samples=4, strides=(1, 2, 2), window_shape=(1, 4, 4), filter_percentile=None):\n",
        "    \"\"\"\n",
        "    Function that uses Occlusion to explain model predictions.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model to explain.\n",
        "        test_loader: DataLoader for test data.\n",
        "        n_samples: Number of samples to analyze.\n",
        "        strides (tuple): Step size for the sliding window (depth, height, width).\n",
        "        window_shape (tuple): Shape of the sliding window (depth, height, width).\n",
        "        filter_percentile (float, optional): Percentile threshold for visualization filtering.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Use reusable function to pick random images\n",
        "    images, labels = pick_random_images_from_loader(test_loader, n_samples)\n",
        "\n",
        "    # Occlusion initialization\n",
        "    occlusion = Occlusion(model)\n",
        "\n",
        "    # Analyze each selected image\n",
        "    for i in range(min(n_samples, len(images))):\n",
        "        img_tensor = images[i]\n",
        "        true_label = labels[i].item() # Convert label tensor to int\n",
        "\n",
        "        # Use reusable function for image preprocessing\n",
        "        img_tensor_unsqueezed, orig_img_np, _ = preprocess_image_for_xai(img_tensor)\n",
        "        img_tensor_unsqueezed = img_tensor_unsqueezed.to(device) # Move input tensor to the correct device\n",
        "\n",
        "        # Use reusable function for model prediction\n",
        "        predicted_class, confidence, _ = get_model_prediction(model, img_tensor_unsqueezed)\n",
        "\n",
        "        print(f\"Processing image {i+1}/{n_samples} - True: {true_label}, Predicted: {predicted_class}\")\n",
        "\n",
        "        # Calculate Occlusion attribution\n",
        "        occlusion_attr = occlusion.attribute(\n",
        "            img_tensor_unsqueezed, # Use the preprocessed tensor\n",
        "            target=predicted_class,\n",
        "            strides=strides,\n",
        "            sliding_window_shapes=window_shape,\n",
        "            baselines=-1.0 # Use black as baseline\n",
        "        )\n",
        "\n",
        "        # Use the reusable display function to visualize the results\n",
        "        display_xai_explanation(\n",
        "            original_image_np=orig_img_np, # Use the preprocessed numpy image\n",
        "            attribution_map=occlusion_attr.abs().squeeze().cpu().detach().numpy(), # Use absolute values and ensure 2D numpy\n",
        "            true_label=true_label,\n",
        "            predicted_class=predicted_class,\n",
        "            technique_title='Occlusion Map',\n",
        "            cmap='hot',\n",
        "            overlay_alpha=0.5,\n",
        "            show_colorbar=True,\n",
        "            filter_percentile=filter_percentile, # Pass filter setting\n",
        "            use_divergent=False # Occlusion map is typically positive\n",
        "        )\n",
        "\n",
        "\n",
        "# Test the refactored function\n",
        "visualize_batch_occlusion(model, test_loader, n_samples=4, filter_percentile=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - GRAD-CAM\n",
        "Baseline of XAI models in CV\n",
        "\n",
        "Grad-CAM answers the question: \"Which regions of the convolutional feature maps contribute the most to the prediction for a specific class?\"\n",
        "\n",
        "Concept: It uses the gradients of the target class with respect to the feature maps of the last convolutional layer to produce a localization map that highlights the important regions in the image.\n",
        "\n",
        "It starts from the concept of CAM (taking the output from the last convolutional filter and training a linear model) and, instead of training a new model (training data isn't always available), it finds the coefficients through backpropagation from the fully connected layers.\n",
        "\n",
        "pros:\n",
        "- works really well\n",
        "\n",
        "cons:\n",
        "- Detail problem (I would need higher resolution for finer visualizations).\n",
        "\n",
        "- It only works with neural networks"
      ],
      "metadata": {
        "id": "d0tjg4CW7iP6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3949202d"
      },
      "source": [
        "def visualize_gradcam_examples(model, test_loader, n_samples=4, filter_percentile=None):\n",
        "\n",
        "    \"\"\"\n",
        "    Function that uses Grad-CAM to explain model predictions.\n",
        "    Args:\n",
        "        model: PyTorch model to explain.\n",
        "        test_loader: DataLoader for test data.\n",
        "        n_samples: Number of samples to analyze.\n",
        "        filter_percentile (float, optional): Percentile threshold for visualization filtering.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # pick random images\n",
        "    images, labels = pick_random_images_from_loader(test_loader, n_samples)\n",
        "\n",
        "    # Grad-CAM Initialization\n",
        "    try:\n",
        "        target_layer = model.conv1\n",
        "        layer_gradcam = LayerGradCam(model, target_layer)\n",
        "    except AttributeError:\n",
        "        print(\"Attention: Target layer not found. Grad-CAM disabled.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    # Analyze each selected image\n",
        "    for i in range(min(n_samples, len(images))):\n",
        "        img_tensor = images[i]\n",
        "        true_label = labels[i].item() # Convert label tensor to int\n",
        "\n",
        "        # image preprocessing\n",
        "        img_tensor_unsqueezed, orig_img_np, _ = preprocess_image_for_xai(img_tensor)\n",
        "        img_tensor_unsqueezed = img_tensor_unsqueezed.to(device) # Move input tensor to the correct device\n",
        "\n",
        "        # model prediction\n",
        "        predicted_class, confidence, _ = get_model_prediction(model, img_tensor_unsqueezed)\n",
        "\n",
        "        print(f\"Processing image {i+1}/{n_samples} - True: {true_label}, Predicted: {predicted_class}\")\n",
        "\n",
        "        # Calculate Grad-CAM attribution\n",
        "        gradcam_attr = layer_gradcam.attribute(img_tensor_unsqueezed, target=predicted_class)\n",
        "\n",
        "        # Resize Grad-CAM map to match original image dimensions (28x28)\n",
        "        gradcam_map = F.interpolate(\n",
        "            gradcam_attr,\n",
        "            size=(28, 28),\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        ).squeeze().cpu().detach().numpy()\n",
        "\n",
        "\n",
        "        # visualize the results\n",
        "        display_xai_explanation(\n",
        "            original_image_np=orig_img_np, # Use the preprocessed numpy image\n",
        "            attribution_map=gradcam_map, # Use the resized Grad-CAM map\n",
        "            true_label=true_label,\n",
        "            predicted_class=predicted_class,\n",
        "            technique_title='Grad-CAM',\n",
        "            cmap='hot',\n",
        "            overlay_alpha=0.5,\n",
        "            show_colorbar=True,\n",
        "            filter_percentile=filter_percentile, # Pass filter setting\n",
        "            use_divergent=False # Grad-CAM typically focuses on positive contributions\n",
        "        )\n",
        "\n",
        "\n",
        "visualize_gradcam_examples(model, test_loader, n_samples=3, filter_percentile=None) # no filter, better results but background with colors\n",
        "\n",
        "visualize_gradcam_examples(model, test_loader, n_samples=3, filter_percentile=80) # filter, worse results, but black background"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XAI Techniques comparison and final considerations"
      ],
      "metadata": {
        "id": "eEAkNE88eXhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_all_xai_techniques(\n",
        "    model,\n",
        "    test_loader,\n",
        "    n_samples=3,\n",
        "    lime_segmentation_method='grid',\n",
        "    grid_size=7,\n",
        "    n_background_shap=50,\n",
        "    n_samples_shap=500,\n",
        "    visualization_mode='all',  # 'raw', 'filtered', 'positive', 'all'\n",
        "    percentile_threshold=85\n",
        "):\n",
        "    \"\"\"\n",
        "    Unified visual comparison between all XAI techniques with proper divergent support.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model.\n",
        "        test_loader: DataLoader for test data.\n",
        "        n_samples (int): Number of samples to analyze.\n",
        "        lime_segmentation_method (str): 'grid' or 'quickshift' for LIME.\n",
        "        grid_size (int): Grid size for LIME segmentation.\n",
        "        n_background_shap (int): Number of background samples for SHAP.\n",
        "        n_samples_shap (int): Number of samples for SHAP.\n",
        "        visualization_mode (str): 'raw', 'filtered', 'positive', or 'all'.\n",
        "        percentile_threshold (int): Threshold percentile for filtering (0-100).\n",
        "    \"\"\"\n",
        "\n",
        "    if visualization_mode not in ['raw', 'filtered', 'positive', 'all']:\n",
        "        raise ValueError(\"visualization_mode must be 'raw', 'filtered', 'positive', or 'all'\")\n",
        "\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    # 1. DATA PREPARATION\n",
        "    total_images_needed = n_samples + n_background_shap\n",
        "    all_images, all_labels = pick_random_images_from_loader(test_loader, total_images_needed)\n",
        "\n",
        "    test_images = all_images[:n_samples]\n",
        "    test_labels = all_labels[:n_samples]\n",
        "    background_images_shap = all_images[n_samples:n_samples + n_background_shap]\n",
        "\n",
        "    # 2. XAI MODELS PREPARATION\n",
        "    saliency = Saliency(model)\n",
        "    occlusion = Occlusion(model)\n",
        "    integrated_gradients = IntegratedGradients(model)\n",
        "\n",
        "    # Prepare Grad-CAM\n",
        "    layer_gradcam = None\n",
        "    try:\n",
        "        target_layer = model.conv1\n",
        "        layer_gradcam = LayerGradCam(model, target_layer)\n",
        "    except AttributeError:\n",
        "        print(\"Attention: Target layer (e.g., model.conv1) not found for Grad-CAM.\")\n",
        "\n",
        "    gradient_shap = GradientShap(model)\n",
        "    baselines_gs = torch.randn_like(test_images) * 0.1\n",
        "    baselines_gs = baselines_gs.to(device)\n",
        "\n",
        "    # Prepare LIME helpers\n",
        "    explainer_lime, lime_predict_fn, segmenter_lime = make_lime_helpers(\n",
        "        model, segmentation_method=lime_segmentation_method, grid_size=grid_size\n",
        "    )\n",
        "\n",
        "    # Prepare Kernel SHAP explainer\n",
        "    background_data_flat_shap = background_images_shap.cpu().numpy().reshape(n_background_shap, -1)\n",
        "    def shap_predict_fn_wrapper(x_flat):\n",
        "        x_tensor = torch.FloatTensor(x_flat).reshape(-1, 1, 28, 28).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(x_tensor)\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "        return probabilities.cpu().numpy()\n",
        "    explainer_shap = shap.KernelExplainer(shap_predict_fn_wrapper, background_data_flat_shap)\n",
        "\n",
        "    # Helper functions for plotting\n",
        "    def plot_attribution_map(ax, raw_map, title, cmap='hot', use_divergent=False,\n",
        "                            apply_filter=False, filter_percentile=85, positive_only=False):\n",
        "        \"\"\"\n",
        "        Helper to plot attribution maps with optional filtering, divergent handling, and positive-only mode.\n",
        "        \"\"\"\n",
        "        # Apply positive-only filtering first if requested\n",
        "        if positive_only:\n",
        "            display_map = raw_map.copy()\n",
        "            display_map[display_map < 0] = 0  # Set negative values to 0\n",
        "            use_divergent = False  # Force non-divergent for positive-only\n",
        "            cmap = 'hot'  # Force hot colormap for positive-only\n",
        "        else:\n",
        "            display_map = raw_map.copy()\n",
        "\n",
        "        # Apply percentile filtering\n",
        "        if apply_filter:\n",
        "            if use_divergent and not positive_only:\n",
        "                abs_map = np.abs(display_map)\n",
        "                threshold = np.percentile(abs_map, filter_percentile)\n",
        "                important_mask = abs_map > threshold\n",
        "                filtered_map = np.zeros_like(display_map)\n",
        "                if important_mask.any():\n",
        "                    important_abs_values = abs_map[important_mask]\n",
        "                    scale_factor = (important_abs_values - threshold) / (important_abs_values.max() - threshold + 1e-8)\n",
        "                    filtered_map[important_mask] = np.sign(display_map[important_mask]) * scale_factor\n",
        "                display_map = filtered_map\n",
        "            else:\n",
        "                abs_map = np.abs(display_map)\n",
        "                threshold = np.percentile(abs_map, filter_percentile)\n",
        "                important_mask = abs_map > threshold\n",
        "                filtered_map = np.zeros_like(abs_map)\n",
        "                if important_mask.any():\n",
        "                    important_values = abs_map[important_mask]\n",
        "                    filtered_map[important_mask] = (important_values - threshold) / (important_values.max() - threshold + 1e-8)\n",
        "                display_map = filtered_map\n",
        "        else:\n",
        "            if not use_divergent or positive_only:\n",
        "                abs_map = np.abs(display_map)\n",
        "                if abs_map.max() > abs_map.min():\n",
        "                    display_map = (abs_map - abs_map.min()) / (abs_map.max() - abs_map.min())\n",
        "                else:\n",
        "                    display_map = abs_map\n",
        "\n",
        "        # Set color range\n",
        "        if use_divergent and not positive_only:\n",
        "            abs_max = max(abs(display_map.min()), abs(display_map.max()))\n",
        "            if abs_max > 0:\n",
        "                vmin, vmax = -abs_max, abs_max\n",
        "            else:\n",
        "                vmin, vmax = 0, 1\n",
        "        else:\n",
        "            vmin, vmax = display_map.min(), display_map.max()\n",
        "\n",
        "        im = ax.imshow(display_map, cmap=cmap, vmin=vmin, vmax=vmax, interpolation='nearest')\n",
        "        return im\n",
        "\n",
        "    def create_visualization(mode_name, apply_filter, positive_only=False):\n",
        "        \"\"\"Create a single visualization (raw, filtered, or positive-only)\"\"\"\n",
        "        n_cols = 8\n",
        "        fig, axes = plt.subplots(n_samples, n_cols, figsize=(4.5 * n_cols, 4 * n_samples))\n",
        "        if n_samples == 1:\n",
        "            axes = axes.reshape(1, -1)\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            img_tensor = test_images[i]\n",
        "            true_label = test_labels[i].item()\n",
        "\n",
        "            img_tensor_unsqueezed, orig_img_np, image_rgb_uint8 = preprocess_image_for_xai(img_tensor)\n",
        "            img_tensor_device = img_tensor_unsqueezed.to(device)\n",
        "\n",
        "            predicted_class, confidence, _ = get_model_prediction(model, img_tensor_device.detach())\n",
        "\n",
        "            # 1. Original image\n",
        "            color = 'green' if predicted_class == true_label else 'red'\n",
        "            axes[i, 0].imshow(orig_img_np, cmap='gray')\n",
        "            axes[i, 0].set_title(f'Originale\\nTrue: {true_label}, Pred: {predicted_class}', color=color, fontsize=10)\n",
        "            axes[i, 0].axis('off')\n",
        "\n",
        "            # 2. Saliency Map\n",
        "            saliency_input = img_tensor_device.detach().requires_grad_(True)\n",
        "            saliency_attr = saliency.attribute(saliency_input, target=predicted_class)\n",
        "            base_cmap = 'hot' if positive_only else 'RdBu_r'\n",
        "            use_div = not positive_only\n",
        "            im = plot_attribution_map(axes[i, 1], saliency_attr.squeeze().cpu().detach().numpy(),\n",
        "                                    '1. Saliency', cmap=base_cmap, use_divergent=use_div,\n",
        "                                    apply_filter=apply_filter, filter_percentile=percentile_threshold,\n",
        "                                    positive_only=positive_only)\n",
        "            fig.colorbar(im, ax=axes[i, 1], shrink=0.7)\n",
        "            axes[i, 1].set_title('1. Saliency', fontsize=10)\n",
        "            axes[i, 1].axis('off')\n",
        "\n",
        "            # 3. Occlusion\n",
        "            occlusion_attr = occlusion.attribute(img_tensor_device.detach(), target=predicted_class,\n",
        "                                               strides=(1, 2, 2), sliding_window_shapes=(1, 4, 4), baselines=-1.0)\n",
        "            im = plot_attribution_map(axes[i, 2], occlusion_attr.squeeze().cpu().detach().numpy(),\n",
        "                                    '2. Occlusion', cmap='hot', use_divergent=False,\n",
        "                                    apply_filter=apply_filter, filter_percentile=percentile_threshold,\n",
        "                                    positive_only=positive_only)\n",
        "            fig.colorbar(im, ax=axes[i, 2], shrink=0.7)\n",
        "            axes[i, 2].set_title('2. Occlusion', fontsize=10)\n",
        "            axes[i, 2].axis('off')\n",
        "\n",
        "            # 4. LIME (special handling for positive_only)\n",
        "            if positive_only:\n",
        "                # For LIME in positive mode, we use positive_only=True\n",
        "                explanation_lime = explainer_lime.explain_instance(image_rgb_uint8, lime_predict_fn,\n",
        "                                                                 top_labels=10, hide_color=0, num_samples=1000,\n",
        "                                                                 segmentation_fn=segmenter_lime)\n",
        "                min_weight = 0.01 if apply_filter else 0.0\n",
        "                _, mask_lime = explanation_lime.get_image_and_mask(predicted_class, positive_only=True,\n",
        "                                                                 num_features=10, hide_rest=False, min_weight=min_weight)\n",
        "            else:\n",
        "                explanation_lime = explainer_lime.explain_instance(image_rgb_uint8, lime_predict_fn,\n",
        "                                                                 top_labels=10, hide_color=0, num_samples=1000,\n",
        "                                                                 segmentation_fn=segmenter_lime)\n",
        "                min_weight = 0.01 if apply_filter else 0.0\n",
        "                _, mask_lime = explanation_lime.get_image_and_mask(predicted_class, positive_only=False,\n",
        "                                                                 num_features=10, hide_rest=False, min_weight=min_weight)\n",
        "\n",
        "            axes[i, 3].imshow(label2rgb(mask_lime, bg_label=0, colors=['red']), interpolation='nearest')\n",
        "            axes[i, 3].set_title(f'3. LIME ({lime_segmentation_method})', fontsize=10)\n",
        "            axes[i, 3].axis('off')\n",
        "\n",
        "            # 5. Kernel SHAP\n",
        "            img_flat = img_tensor_unsqueezed.detach().cpu().numpy().reshape(1, -1)\n",
        "            shap_values = explainer_shap.shap_values(img_flat, nsamples=n_samples_shap)\n",
        "            shap_for_class = shap_values[predicted_class][0] if isinstance(shap_values, list) else shap_values[0, :, predicted_class]\n",
        "            shap_map = shap_for_class.reshape(28, 28)\n",
        "            base_cmap = 'hot' if positive_only else 'RdBu_r'\n",
        "            use_div = not positive_only\n",
        "            im = plot_attribution_map(axes[i, 4], shap_map, '4. Kernel SHAP', cmap=base_cmap,\n",
        "                                    use_divergent=use_div, apply_filter=apply_filter,\n",
        "                                    filter_percentile=percentile_threshold, positive_only=positive_only)\n",
        "            fig.colorbar(im, ax=axes[i, 4], shrink=0.7)\n",
        "            axes[i, 4].set_title('4. Kernel SHAP', fontsize=10)\n",
        "            axes[i, 4].axis('off')\n",
        "\n",
        "            # 6. GradientSHAP\n",
        "            gs_attr = gradient_shap.attribute(img_tensor_device.detach(),\n",
        "                                            baselines=baselines_gs[:min(len(baselines_gs), 10)],\n",
        "                                            target=predicted_class, n_samples=50)\n",
        "            base_cmap = 'hot' if positive_only else 'RdBu_r'\n",
        "            use_div = not positive_only\n",
        "            im = plot_attribution_map(axes[i, 5], gs_attr.squeeze().cpu().detach().numpy(),\n",
        "                                    '5. GradientSHAP', cmap=base_cmap, use_divergent=use_div,\n",
        "                                    apply_filter=apply_filter, filter_percentile=percentile_threshold,\n",
        "                                    positive_only=positive_only)\n",
        "            fig.colorbar(im, ax=axes[i, 5], shrink=0.7)\n",
        "            axes[i, 5].set_title('5. GradientSHAP', fontsize=10)\n",
        "            axes[i, 5].axis('off')\n",
        "\n",
        "            # 7. Grad-CAM\n",
        "            if layer_gradcam:\n",
        "                gradcam_attr = layer_gradcam.attribute(img_tensor_device.detach(), target=predicted_class)\n",
        "                gradcam_map = F.interpolate(gradcam_attr, size=(28, 28), mode='bilinear', align_corners=False)\n",
        "                gradcam_map = gradcam_map.squeeze().cpu().detach().numpy()\n",
        "                im = plot_attribution_map(axes[i, 6], gradcam_map, '6. Grad-CAM', cmap='hot',\n",
        "                                        use_divergent=False, apply_filter=apply_filter,\n",
        "                                        filter_percentile=percentile_threshold, positive_only=positive_only)\n",
        "                fig.colorbar(im, ax=axes[i, 6], shrink=0.7)\n",
        "                axes[i, 6].set_title('6. Grad-CAM', fontsize=10)\n",
        "                axes[i, 6].axis('off')\n",
        "            else:\n",
        "                axes[i, 6].text(0.5, 0.5, 'Grad-CAM\\nDisabled', ha='center', va='center', fontsize=10)\n",
        "                axes[i, 6].axis('off')\n",
        "\n",
        "            # 8. Integrated Gradients\n",
        "            baseline_ig = torch.zeros_like(img_tensor_device).to(device) - 1.0\n",
        "            ig_attr = integrated_gradients.attribute(img_tensor_device.detach(), baselines=baseline_ig,\n",
        "                                                   target=predicted_class, n_steps=50)\n",
        "            base_cmap = 'hot' if positive_only else 'RdBu_r'\n",
        "            use_div = not positive_only\n",
        "            im = plot_attribution_map(axes[i, 7], ig_attr.squeeze().cpu().detach().numpy(),\n",
        "                                    '7. Integrated Grads', cmap=base_cmap, use_divergent=use_div,\n",
        "                                    apply_filter=apply_filter, filter_percentile=percentile_threshold,\n",
        "                                    positive_only=positive_only)\n",
        "            fig.colorbar(im, ax=axes[i, 7], shrink=0.7)\n",
        "            axes[i, 7].set_title('7. Integrated Grads', fontsize=10)\n",
        "            axes[i, 7].axis('off')\n",
        "\n",
        "        # Create appropriate title based on mode\n",
        "        if positive_only:\n",
        "            filter_text = f\" (Positive Only, Filtered at {percentile_threshold}%)\" if apply_filter else \" (Positive Only)\"\n",
        "        else:\n",
        "            filter_text = f\" (Filtered at {percentile_threshold}%)\" if apply_filter else \" (Raw Data)\"\n",
        "\n",
        "        plt.suptitle(f'Comparison between all XAI Techniques{filter_text}', fontsize=16, y=1.0)\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
        "        plt.show()\n",
        "\n",
        "    # Execute based on visualization_mode\n",
        "    if visualization_mode == 'raw':\n",
        "        create_visualization('Raw', apply_filter=False, positive_only=False)\n",
        "    elif visualization_mode == 'filtered':\n",
        "        create_visualization('Filtered', apply_filter=True, positive_only=False)\n",
        "    elif visualization_mode == 'positive':\n",
        "        create_visualization('Positive Only', apply_filter=True, positive_only=True)\n",
        "    elif visualization_mode == 'all':\n",
        "        print(\"=== RAW VISUALIZATION ===\")\n",
        "        create_visualization('Raw', apply_filter=False, positive_only=False)\n",
        "        print(\"\\n=== FILTERED VISUALIZATION ===\")\n",
        "        create_visualization('Filtered', apply_filter=True, positive_only=False)\n",
        "        print(\"\\n=== FILTERED VISUALIZATION (POSITIVE ONLY) ===\")\n",
        "        create_visualization('Positive Only', apply_filter=True, positive_only=True)\n",
        "\n",
        "\n",
        "# Esempio di utilizzo con tutte le modalità di visualizzazione\n",
        "compare_all_xai_techniques(model, test_loader, n_samples=3, visualization_mode='all', percentile_threshold=85)"
      ],
      "metadata": {
        "id": "DtaP_hjbBoOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Techniques comparison discussion (refered to mnist dataset)**\n",
        "\n",
        "1. Saliency\n",
        "Gradient based, fast and simple to implement, but attributions are noisy.\n",
        "It identifies the more important pixels but often result in a confusing shape, good as baseline\n",
        "\n",
        "2. Occlusion  \n",
        "Perturbation-based method, it evaluates \"blocks\" of the image, excels on high-contrast datasets like MNIST, identifying which parts of the digit are most critical for the prediction. The output can be blurry due to the window size, but its explanation is one of the most intuitive and reliable for this task.\n",
        "\n",
        "3. LIME (GRID SEGMENTATION ALGORITHM)  \n",
        "Provides explanation based on superpixels. The rigid grid segmentation captures very well the structure, but has low precision (rigid blocks not aligned with digits)\n",
        "\n",
        "4. Kernel SHAP  \n",
        "Model-agnostic, highlights sparse pixels but loses spatial coherence since it treats features independently. Difficult to interpret on images, better suited to tabular data.\n",
        "\n",
        "5. GradientSHAP\n",
        "Improves over Saliency by averaging gradients from multiple baselines, but in this dataset it has more or less the same performance of Saliency, it's better to use other methods\n",
        "\n",
        "6. Grad-CAM  \n",
        "This technique produces a class-activation map that is excellent for localization, providing a very clean and understandable explanation of the model's attention.\n",
        "Heatmaps are low-resolution, as it's derived from the final convolutional layer, but overall is one of the best techniques to use every time we work woth images\n",
        "\n",
        "7. Integrated Gradients\n",
        "Captures continuous contribution along the baseline-to-input path. Results are sharp, follow digit contours well, and offer the one of the best explanations for MNIST among all methods.\n",
        "\n",
        "**Conclusion**  \n",
        "Overall, all the techniques shown provided some insights of what the model \"looks\" when making a prediction, but I have to say that on **MNIST**, using my model and XAI techniques setup, **Integrated Gradients** provide the clearest and most faithful attributions. **Grad-CAM** is very good for localization but provides low resolution images. **Occlusion** is intuitive and reliable but a little blurry."
      ],
      "metadata": {
        "id": "4Uw7ReoUI1qI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3: Error analysis"
      ],
      "metadata": {
        "id": "OX8cMZO5yQKn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6a87d27"
      },
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Create empty lists to store misclassified images, true labels, and predicted labels\n",
        "misclassified_images = []\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Iterate through the test loader\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Move images and labels to the device\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Get model predictions\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Determine the predicted class for each image\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Identify misclassified images\n",
        "        misclassified_mask = (predicted != labels)\n",
        "\n",
        "        # Store misclassified images and their labels\n",
        "        misclassified_images.extend(images[misclassified_mask].cpu())\n",
        "        true_labels.extend(labels[misclassified_mask].cpu())\n",
        "        predicted_labels.extend(predicted[misclassified_mask].cpu())\n",
        "\n",
        "print(f\"Found {len(misclassified_images)} misclassified images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_and_visualize_misclassified(model, misclassified_images, true_labels, predicted_labels,\n",
        "                                      preprocess_image_for_xai, max_images=5,\n",
        "                                      target_layer_name='conv1', n_steps=50):\n",
        "    \"\"\"\n",
        "    Analyzes and visualizes misclassified images using Grad-CAM and Integrated Gradients.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model to analyze.\n",
        "        misclassified_images: List of misclassified images.\n",
        "        true_labels: List of true labels for the misclassified images.\n",
        "        predicted_labels: List of predicted labels for the misclassified images.\n",
        "        preprocess_image_for_xai: Function to preprocess the image for XAI.\n",
        "        max_images: Maximum number of images to visualize.\n",
        "        target_layer_name: Name of the target layer for Grad-CAM.\n",
        "        n_steps: Number of steps for Integrated Gradients.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (gradcam_results, ig_results) containing the results of Grad-CAM and Integrated Gradients.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Randomly select images to analyze\n",
        "    import random\n",
        "    total_images = len(misclassified_images)\n",
        "    num_to_analyze = min(max_images, total_images)\n",
        "\n",
        "    if total_images > num_to_analyze:\n",
        "        selected_indices = random.sample(range(total_images), num_to_analyze)\n",
        "    else:\n",
        "        selected_indices = list(range(num_to_analyze))\n",
        "\n",
        "    # Initialize Grad-CAM\n",
        "    target_layer = getattr(model, target_layer_name)\n",
        "    layer_gradcam = LayerGradCam(model, target_layer)\n",
        "\n",
        "    # Initialize Integrated Gradients\n",
        "    ig = IntegratedGradients(model)\n",
        "\n",
        "    # Lists to store results\n",
        "    gradcam_results = []\n",
        "    ig_results = []\n",
        "\n",
        "    # Process each selected misclassified image\n",
        "    for idx in selected_indices:\n",
        "        img_tensor = misclassified_images[idx]\n",
        "        true_label = true_labels[idx].item()\n",
        "        predicted_label = predicted_labels[idx].item()\n",
        "\n",
        "        # Preprocess image for XAI analysis\n",
        "        img_tensor_unsqueezed, orig_img_np, _ = preprocess_image_for_xai(img_tensor)\n",
        "        img_tensor_device = img_tensor_unsqueezed.to(device)\n",
        "\n",
        "        # Generate Grad-CAM attribution for predicted class\n",
        "        gradcam_attr = layer_gradcam.attribute(img_tensor_device, target=predicted_label)\n",
        "\n",
        "        # Resize Grad-CAM map to match input image size\n",
        "        gradcam_map = F.interpolate(\n",
        "            gradcam_attr,\n",
        "            size=(28, 28),\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        ).squeeze().cpu().detach().numpy()\n",
        "\n",
        "\n",
        "        # Store Grad-CAM results\n",
        "        gradcam_results.append({\n",
        "            'original_image_np': orig_img_np,\n",
        "            'true_label': true_label,\n",
        "            'predicted_label': predicted_label,\n",
        "            'gradcam_map': gradcam_map\n",
        "        })\n",
        "\n",
        "        # Set baseline for Integrated Gradients (black image)\n",
        "        baseline = torch.zeros_like(img_tensor_device).to(device) - 1.0\n",
        "\n",
        "        # Generate Integrated Gradients attribution for predicted class\n",
        "        ig_attr = ig.attribute(\n",
        "            img_tensor_device,\n",
        "            baselines=baseline,\n",
        "            target=predicted_label,\n",
        "            n_steps=n_steps\n",
        "        )\n",
        "\n",
        "        # Convert to numpy array\n",
        "        ig_map = ig_attr.squeeze().cpu().detach().numpy()\n",
        "\n",
        "        # Store Integrated Gradients results\n",
        "        ig_results.append({\n",
        "            'original_image_np': orig_img_np,\n",
        "            'true_label': true_label,\n",
        "            'predicted_label': predicted_label,\n",
        "            'ig_map': ig_map\n",
        "        })\n",
        "\n",
        "\n",
        "    # Visualize the selected images\n",
        "    for i, idx in enumerate(selected_indices):\n",
        "        # Get data for current image\n",
        "        gradcam_data = gradcam_results[i]\n",
        "        ig_data = ig_results[i]\n",
        "\n",
        "        original_image_np = gradcam_data['original_image_np']\n",
        "        true_label = gradcam_data['true_label']\n",
        "        predicted_label = gradcam_data['predicted_label']\n",
        "        gradcam_map = gradcam_data['gradcam_map']\n",
        "        ig_map = ig_data['ig_map']\n",
        "\n",
        "        # Create visualization with 3 columns: Original, Grad-CAM, Integrated Gradients\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "        # 1. Original image\n",
        "        axes[0].imshow(original_image_np, cmap='gray')\n",
        "        axes[0].set_title(f'Original (True: {true_label}, Pred: {predicted_label})', color='red')\n",
        "        axes[0].axis('off')\n",
        "\n",
        "        # 2. Grad-CAM heatmap\n",
        "        im_gradcam = axes[1].imshow(gradcam_map, cmap='hot', interpolation='nearest')\n",
        "        axes[1].set_title('Grad-CAM')\n",
        "        axes[1].axis('off')\n",
        "        fig.colorbar(im_gradcam, ax=axes[1])\n",
        "\n",
        "        # 3. Integrated Gradients map (absolute values)\n",
        "        im_ig = axes[2].imshow(np.abs(ig_map), cmap='hot', interpolation='nearest')\n",
        "        axes[2].set_title('Integrated Gradients')\n",
        "        axes[2].axis('off')\n",
        "        fig.colorbar(im_ig, ax=axes[2])\n",
        "\n",
        "        plt.suptitle(f'True: {true_label}, Predicted: {predicted_label}',\n",
        "                     fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return gradcam_results, ig_results\n",
        "\n",
        "\n",
        "\n",
        "gradcam_results, ig_results = analyze_and_visualize_misclassified(\n",
        "    model=model,\n",
        "    misclassified_images=misclassified_images,\n",
        "    true_labels=true_labels,\n",
        "    predicted_labels=predicted_labels,\n",
        "    preprocess_image_for_xai=preprocess_image_for_xai,\n",
        "    max_images=5,\n",
        "    target_layer_name='conv1',\n",
        "    n_steps=50\n",
        ")"
      ],
      "metadata": {
        "id": "wZ5SC2gmN6Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Error Analysis Observations:**\n",
        "\n",
        "The inspection of misclassified samples with Grad-CAM and Integrated Gradients shows that the model generally attends to the digit itself rather than irrelevant background. Most errors arise from poorly written digits, which are ambiguous to understand even for humans.\n",
        "\n",
        "In other cases, misclassifications comes from digit overlap in handwriting styles. For example, a \"9\" may resemble a \"4\" depending on how it is drawn, and a poorly closed \"8\" can resemble a \"0\" with background noise. These confusions highlight the natural variability of handwritten input rather than a clear model failure.\n",
        "\n",
        "To address such issues, two strategies can be considered:\n",
        "\n",
        "- Data cleaning: reducing or filtering problematic samples with severe ambiguity.\n",
        "\n",
        "- Data enrichment (recommended): adding more representative samples of ambiguous digit styles to improve model robustness."
      ],
      "metadata": {
        "id": "ZCy6Tj7V1gnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3 (optional): Full explainable system that can make the same classification, offering further insights on model decision"
      ],
      "metadata": {
        "id": "KZCivavsRomH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f1600c2"
      },
      "source": [
        "# Converting PyTorch tensors to NumPy arrays and flatten the images\n",
        "X_train = train_dataset.data.numpy().reshape(-1, 28 * 28)\n",
        "y_train = train_dataset.targets.numpy()\n",
        "X_test = test_dataset.data.numpy().reshape(-1, 28 * 28)\n",
        "y_test = test_dataset.targets.numpy()\n",
        "\n",
        "# Training a Decision Tree Classifier\n",
        "dt_model = DecisionTreeClassifier(max_depth=8, random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = dt_model.score(X_test, y_test)\n",
        "print(f\"\\nDecision Tree Classifier Accuracy on test set: {accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3710d9cb"
      },
      "source": [
        "### Visualizing Decision Tree Decisions\n",
        "\n",
        "To illustrate how the Decision Tree makes predictions, we can visualize a small part of the tree or trace the decision path for a specific image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d14cccb"
      },
      "source": [
        "# Plot the Decision Tree Classifier (truncated for readability)\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(dt_model, max_depth=3, feature_names=[f'pixel_{i}' for i in range(X_train.shape[1])],\n",
        "          class_names=[classes[i] for i in dt_model.classes_], filled=True, fontsize=8)\n",
        "plt.title('Decision Tree Classifier (Truncated to Depth 3)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the classical tree diagram provides a global view of the model, it is still challenging to understand the sequence of pixel-level rules applied to a single prediction.\n",
        "To make the decision process more transparent, I made a custom function that:\n",
        "\n",
        "- Traces the exact path the decision tree follows for one image.\n",
        "\n",
        "- Translates raw pixel values into simplified categories (Background, Faint Ink, Intense Ink) for easier readability.\n",
        "\n",
        "- Shows the inspected pixels directly on the original image, in the same order they were evaluated.\n",
        "\n",
        "This approach is not perfect, but it provides a practical example of how a white-box model can easily “explain” the reasoning behind a specific prediction."
      ],
      "metadata": {
        "id": "MkAIybIcbdSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_decision_path(image_index, dt_model, X_test, y_test, test_dataset):\n",
        "    \"\"\"\n",
        "    Visualize and explain the decision path of a Decision Tree classifier for a single image.\n",
        "\n",
        "    This function shows:\n",
        "      1. A step-by-step textual explanation of which pixels the tree inspects\n",
        "         along the decision path, with pixel values translated into human-friendly categories\n",
        "         (BACKGROUND, FAINT INK, INTENSE INK).\n",
        "      2. The prediction compared to the true label.\n",
        "      3. A graphical overlay of the inspected pixels on the original image, annotated with\n",
        "         the order in which the tree considered them.\n",
        "\n",
        "    Args:\n",
        "        image_index (int): Index of the test image to analyze.\n",
        "        dt_model (DecisionTreeClassifier): Trained scikit-learn decision tree model.\n",
        "        X_test (np.ndarray): Test feature matrix (flattened images).\n",
        "        y_test (np.ndarray or list): True labels corresponding to the test set.\n",
        "        test_dataset (torchvision Dataset): Dataset containing the original images.\n",
        "\n",
        "    \"\"\"\n",
        "    # Data Preparation and Rule Extraction\n",
        "    image_for_prediction = X_test[image_index]\n",
        "    image_for_visualization = test_dataset.data.numpy()[image_index]\n",
        "    prediction = dt_model.predict([image_for_prediction])[0]\n",
        "    true_label = y_test[image_index]\n",
        "\n",
        "    tree_model = dt_model.tree_\n",
        "    node_indicator = dt_model.decision_path([image_for_prediction])\n",
        "    node_index = node_indicator.indices\n",
        "\n",
        "    feature_indices = []\n",
        "    thresholds = []\n",
        "    for node_id in node_index:\n",
        "        if dt_model.tree_.feature[node_id] != _tree.TREE_UNDEFINED:\n",
        "            feature_idx = dt_model.tree_.feature[node_id]\n",
        "            threshold = dt_model.tree_.threshold[node_id]\n",
        "            feature_indices.append(feature_idx)\n",
        "            thresholds.append(threshold)\n",
        "\n",
        "\n",
        "    for i, feature_idx in enumerate(feature_indices):\n",
        "        threshold = thresholds[i]\n",
        "        pixel_value = image_for_prediction[feature_idx]\n",
        "\n",
        "        # Pixel intensity translation, for a semplified and more readable way\n",
        "        if pixel_value < 85:\n",
        "            pixel_type = \"BACKGROUND (value: 0-84)\"\n",
        "        elif pixel_value < 170:\n",
        "            pixel_type = \"FAINT INK (value: 85-169)\"\n",
        "        else:\n",
        "            pixel_type = \"INTENSE INK (value: 170+)\"\n",
        "\n",
        "\n",
        "        # Print the report combining pixel classification with the exact rule\n",
        "        print(f\"Step {i+1}: Model inspected a {pixel_type} pixel.\")\n",
        "\n",
        "    # model prediction and true label\n",
        "    print(f\"\\nPrediction: {prediction}\")\n",
        "    print(f\"True Label: {true_label}\\n\")\n",
        "\n",
        "    # Graphical Visualization\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(image_for_visualization, cmap='gray')\n",
        "    for i, feature_idx in enumerate(feature_indices):\n",
        "        y = feature_idx // 28\n",
        "        x = feature_idx % 28\n",
        "        color = plt.cm.viridis(i / len(feature_indices))\n",
        "        circle = plt.Circle((x, y), radius=0.8, color=color, fill=False, linewidth=2.5)\n",
        "        plt.gca().add_patch(circle)\n",
        "        plt.text(x + 1.2, y + 0.2, str(i + 1), color=color, fontsize=14, weight='bold')\n",
        "    result_color = 'green' if prediction == true_label else 'red'\n",
        "    plt.setp(plt.gca().spines.values(), color=result_color, linewidth=4)\n",
        "    plt.title(f\"Visual Path for Prediction: '{prediction}', True: '{true_label}'\", fontsize=16, color = result_color)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# let's try with 5 examples\n",
        "num_examples_to_show = 5\n",
        "\n",
        "# setting random test set indices\n",
        "random_indices = random.sample(range(len(X_test)), num_examples_to_show)\n",
        "\n",
        "# Analyze and print each of the selected random images\n",
        "for index in random_indices:\n",
        "    visualize_decision_path(index, dt_model, X_test, y_test, test_dataset)"
      ],
      "metadata": {
        "id": "e7fII23csf80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook conclusions"
      ],
      "metadata": {
        "id": "PBcoXIoU0_p7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook shows that Explainable AI (XAI) is not a single method, but a “world full of wonders\".  \n",
        "Here we explored a selection of techniques, from gradient-based methods to perturbation-based approaches and model-agnostic frameworks, each with its own strengths and limitations.  \n",
        "The goal was not to exhaustively cover every algorithm, but to provide a first comparative overview. I think that each technique (and its many variants) could justify a dedicated notebook (or even more) to fully understand its behavior, parameters, and best use cases.  \n",
        "The key takeaway is that XAI is essential, now more than ever, as the world moves rapidly from simple, transparent white-box models to increasingly complex black-box architectures.  \n",
        "Interpretability is critical for trust, compliance, and responsible AI adoption."
      ],
      "metadata": {
        "id": "eKvMBfmb2SVm"
      }
    }
  ]
}